{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper starts by discussing scaled dot-product attention. *Attention* refers to a mechanism that allows for \"modeling of dependencies without regard to their input or output sequencies\". In other words, attention allows the model to *attend* to different parts of the input when learning to approximate a function.\n",
    "\n",
    "The common example shown for attention is how different words in a sentence are associated each other. For example, consider the sentence \"A big red dog jumped over a small pond\". As a reader, it's easy to understand that the words \"big\", \"red\", and \"jumped\" all refer to the dog, or are at least more relevant to understand what the dog is doing than the word \"small\". Attention allows a model to learn and understand the strength of these associations, allowing it to better understand the context and predict the correct output.\n",
    "\n",
    "> Dot-product attention is much faster and more space-efficient in practice [than additive attention], since it can be implemented using highly optimized matrix multiplication code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor: # TODO: implement masking\n",
    "    \"\"\"\n",
    "    Performs scaled dot-product attention as defined in the Transformers paper.\n",
    "    Assumes that d_keys and d_values are equal to d_model / num_heads.\n",
    "\n",
    "    :param Q: The query vector of shape (num_heads, d_model, d_keys)\n",
    "    :param K: The key vector of shape (num_heads, d_model, d_keys)\n",
    "    :param V: The values vector of shape (num_heads, d_model, d_values)\n",
    "    :return: The scaled attention scores of shape (num_heads, d_model, d_values)\n",
    "    \"\"\"\n",
    "    d_keys = Q.shape[2]\n",
    "\n",
    "    scaling_factor = 1 / math.sqrt(d_keys)\n",
    "\n",
    "    return F.softmax(Q @ torch.transpose(K, 1, 2) * scaling_factor, dim=0) @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512, 64])\n"
     ]
    }
   ],
   "source": [
    "# let's try this out with some random values\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_keys = d_model // num_heads\n",
    "d_values = d_model // num_heads\n",
    "\n",
    "Q = torch.randn((num_heads, d_model, d_keys))\n",
    "K = torch.randn((num_heads, d_model, d_keys))\n",
    "V = torch.randn((num_heads, d_model, d_values))\n",
    "\n",
    "scaled_scores = scaled_dot_product_attention(Q, K, V)\n",
    "print(scaled_scores.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we can start building out the attention layer. Let's see what the paper has to say about an attention head:\n",
    "\n",
    "> Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2.\n",
    "\n",
    "> Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n",
    "\n",
    "> $\\text{MultiHead}(Q,K,V)=\\text{Concat}(\\text{head}_1,...,\\text{head}_h)W^O$\n",
    "> $\\text{where head}_i=\\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements multi-headed attention from the Attention paper.\n",
    "\n",
    "    :param d_model: Also equal the embedding dimension. \"All sub-layers in the model, as well as the embedding layers, produce outputs of dimension d_model\".\n",
    "    :param num_heads: How many attention heads to use. Must be a multiple of d_model.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        assert (d_model % num_heads == 0), \"d_model is not a multiple of num_heads\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_keys = d_model / num_heads\n",
    "        self.d_values = d_model / num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # each head has its own query, key, and value vector; in order to parallelize all of these calculations,\n",
    "        # we batch them by num_heads and perform broadcasted matrix multiplication\n",
    "        self.W_Q = nn.Parameter(torch.zeros((num_heads, d_model, d_keys)))\n",
    "        self.W_K = nn.Parameter(torch.zeros((num_heads, d_model, d_keys)))\n",
    "        self.W_V = nn.Parameter(torch.zeros((num_heads, d_model, d_values)))\n",
    "        \n",
    "        self.W_O = nn.Parameter(torch.zeros((num_heads * d_values, d_model)))\n",
    "        self.b_O = nn.Parameter(torch.zeros((num_heads * d_values)))\n",
    "        # TODO: double check if this is the correct initialization for each parameter\n",
    "\n",
    "        nn.init.xavier_normal_(self.W_Q)\n",
    "        nn.init.xavier_normal_(self.W_K)\n",
    "        nn.init.xavier_normal_(self.W_V)\n",
    "        nn.init.xavier_normal_(self.W_O)\n",
    "        nn.init.xavier_normal_(self.b_O)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = x @ self.W_Q\n",
    "        K = x @ self.W_K\n",
    "        V = x @ self.W_V\n",
    "\n",
    "        Z = scaled_dot_product_attention(Q, K, V) # should be dim (num_heads, d_model, d_values)\n",
    "\n",
    "        # now we need to \"concatenate\" all of the outputs Z along the num_heads axis\n",
    "        Z = Z.reshape(self.num_heads * self.d_model, self.d_values)\n",
    "\n",
    "        # finally, multiply the output by W_O and add the biases\n",
    "        out = Z @ self.W_O + self.b_O\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
