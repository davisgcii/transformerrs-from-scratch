{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper starts by discussing scaled dot-product attention. *Attention* refers to a mechanism that allows for \"modeling of dependencies without regard to their input or output sequencies\". In other words, attention allows the model to *attend* to different parts of the input when learning to approximate a function.\n",
    "\n",
    "The common example shown for attention is how different words in a sentence are associated each other. For example, consider the sentence \"A big red dog jumped over a small pond\". As a reader, it's easy to understand that the words \"big\", \"red\", and \"jumped\" all refer to the dog, or are at least more relevant to understand what the dog is doing than the word \"small\". Attention allows a model to learn and understand the strength of these associations, allowing it to better understand the context and predict the correct output.\n",
    "\n",
    "> Dot-product attention is much faster and more space-efficient in practice [than additive attention], since it can be implemented using highly optimized matrix multiplication code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor: # TODO: implement masking\n",
    "    \"\"\"\n",
    "    Performs scaled dot-product attention as defined in the Transformers paper.\n",
    "    Assumes that d_keys and d_values are equal to d_model / num_heads.\n",
    "\n",
    "    :param Q: The query vector of shape (num_heads, d_model, d_keys)\n",
    "    :param K: The key vector of shape (num_heads, d_model, d_keys)\n",
    "    :param V: The values vector of shape (num_heads, d_model, d_values)\n",
    "    :return: The scaled attention scores of shape (num_heads, d_model, d_values)\n",
    "    \"\"\"\n",
    "    d_keys = Q.shape[2] # assumes that Q has shape (num_heads, d_model, d_keys)\n",
    "\n",
    "    scaling_factor = 1 / math.sqrt(d_keys)\n",
    "\n",
    "    # learning: \"@\" is an alias for torch.matmul()\n",
    "    return F.softmax(Q @ torch.transpose(K, 1, 2) * scaling_factor, dim=0) @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512, 64])\n"
     ]
    }
   ],
   "source": [
    "# let's try this out with some random values\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_keys = d_model // num_heads\n",
    "d_values = d_model // num_heads\n",
    "\n",
    "Q = torch.randn((num_heads, d_model, d_keys))\n",
    "K = torch.randn((num_heads, d_model, d_keys))\n",
    "V = torch.randn((num_heads, d_model, d_values))\n",
    "\n",
    "scaled_scores = scaled_dot_product_attention(Q, K, V)\n",
    "print(scaled_scores.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we can start building out the attention layer. Let's see what the paper has to say about an attention head:\n",
    "\n",
    "> Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2.\n",
    "\n",
    "> Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n",
    "\n",
    "> $\\text{MultiHead}(Q,K,V)=\\text{Concat}(\\text{head}_1,...,\\text{head}_h)W^O$\n",
    "> $\\text{where head}_i=\\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements multi-headed attention from the Attention paper.\n",
    "\n",
    "    :param d_model: Also equal the embedding dimension. \"All sub-layers in the model, as well as the embedding layers, produce outputs of dimension d_model\".\n",
    "    :param num_heads: How many attention heads to use. Must be a multiple of d_model.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model=512, num_heads=8, bias=True, dropout=0.0):\n",
    "        assert (d_model % num_heads == 0), \"d_model is not a multiple of num_heads\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_keys = d_model // num_heads\n",
    "        self.d_values = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.bias = bias\n",
    "\n",
    "        # each head has its own query, key, and value vector; in order to parallelize all of these calculations,\n",
    "        # we batch them by num_heads and perform broadcasted matrix multiplication\n",
    "\n",
    "        # learning: using nn.Parameter here -- nn.Parameter is a submodule of nn.Tensor which, when created inside\n",
    "        # of a module, are added to the list of Module parameters. This helps the module know that these\n",
    "        # parameters should be optimized by the optimizer.\n",
    "        self.W_Q = nn.Parameter(torch.zeros((num_heads, d_model, d_keys)))\n",
    "        self.W_K = nn.Parameter(torch.zeros((num_heads, d_model, d_keys)))\n",
    "        self.W_V = nn.Parameter(torch.zeros((num_heads, d_model, d_values)))\n",
    "        \n",
    "        self.W_O = nn.Parameter(torch.zeros((num_heads * d_values, d_model)))\n",
    "        self.b_O = None\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        if self.bias:\n",
    "            self.b_O = nn.Parameter(torch.zeros((num_heads * d_values)))\n",
    "\n",
    "        # using xavier/glorot initialization based on this post: https://ai.stackexchange.com/questions/30491/is-there-a-proper-initialization-technique-for-the-weight-matrices-in-multi-head\n",
    "        nn.init.xavier_normal_(self.W_Q)\n",
    "        nn.init.xavier_normal_(self.W_K)\n",
    "        nn.init.xavier_normal_(self.W_V)\n",
    "        nn.init.uniform_(self.W_O) # TODO: unsure if this is the right initialization for W_O and b_O\n",
    "        nn.init.uniform_(self.b_O)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = x @ self.W_Q\n",
    "        K = x @ self.W_K\n",
    "        V = x @ self.W_V\n",
    "        print(f\"d_model: {self.d_model}\")\n",
    "        print(f\"d_keys: {self.d_keys}\")\n",
    "        print(f\"d_values: {self.d_values}\")\n",
    "\n",
    "        print(f\"Q shape: {Q.shape}\")\n",
    "        print(f\"K shape: {K.shape}\")\n",
    "        print(f\"V shape: {V.shape}\")\n",
    "\n",
    "        Z = scaled_dot_product_attention(Q, K, V) # should be dim (num_heads, d_model, d_values)\n",
    "        print(f\"Z shape: {Z.shape}\")\n",
    "        print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "        # now we need to \"concatenate\" all of the outputs Z to form one matrix of shape (x.shape[0], d_model)\n",
    "        Z = Z.reshape(x.shape[0], self.d_model)\n",
    "        # learning: Tensor.shape is an alias for Tensor.size()\n",
    "\n",
    "        # finally, multiply the output by W_O and add the biases\n",
    "        out = Z @ self.W_O\n",
    "\n",
    "        if self.bias:\n",
    "            out += self.b_O\n",
    "\n",
    "        print(f\"Output shape: {out.shape}\")\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_model: 512\n",
      "d_keys: 64\n",
      "d_values: 64\n",
      "Q shape: torch.Size([8, 100, 64])\n",
      "K shape: torch.Size([8, 100, 64])\n",
      "V shape: torch.Size([8, 100, 64])\n",
      "Z shape: torch.Size([8, 100, 64])\n",
      "Input shape: torch.Size([100, 512])\n",
      "Output shape: torch.Size([100, 512])\n"
     ]
    }
   ],
   "source": [
    "# let's test this out\n",
    "multi_headed_attention = MultiHeadedAttention()\n",
    "input = torch.randn((100, d_model))\n",
    "\n",
    "out = multi_headed_attention(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
