{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper starts by discussing scaled dot-product attention. *Attention* refers to a mechanism that allows for \"modeling of dependencies without regard to their input or output sequencies\". In other words, attention allows the model to *attend* to different parts of the input when learning to approximate a function.\n",
    "\n",
    "The common example shown for attention is how different words in a sentence relate to each other. For example, consider the sentence \"A big red dog jumped over a small pond\". As a reader, it's easy to understand that the words \"big\", \"red\", and \"jumped\" all refer to the dog, or are at least more relevant to understand what the dog is doing than the word \"small\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Performs scaled dot-product attention as defined in the Transformers paper.\n",
    "\n",
    "    :param Q: The query vector of shape (d_model, d_keys)\n",
    "    :param K: The key vector of shape (d_model, d_keys)\n",
    "    :param V: The values vector of shape (d_model, d_values)\n",
    "    :return: The scaled attention scores of shape (d_model, d_values)\n",
    "    \"\"\"\n",
    "    d_keys = Q.shape[1]\n",
    "\n",
    "    scaling_factor = 1 / math.sqrt(d_keys)\n",
    "\n",
    "    return F.softmax(Q @ K.T * scaling_factor, dim=0) @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try this out with some random values\n",
    "d_keys = 100\n",
    "d_values = 25\n",
    "d_model = 1000\n",
    "\n",
    "Q = torch.randn((d_model, d_keys))\n",
    "K = torch.randn((d_model, d_keys))\n",
    "V = torch.randn((d_model, d_values))\n",
    "\n",
    "scaled_scores = scaled_dot_product_attention(Q, K, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 25])\n"
     ]
    }
   ],
   "source": [
    "print(scaled_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
